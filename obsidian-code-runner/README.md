# Obsidian Code Runner

**Run code blocks (Python/JS) and LLM prompts directly inside Obsidian, Jupyter-style.**

Transform your Obsidian vault into a computational notebook with persistent Python sessions, inline execution, and AI integration.

---

## âœ¨ Features

### Core Execution
- âœ… **Python & JavaScript** - Run code blocks in reading and editor mode
- âœ… **Kernel Mode** - Persistent Python sessions (variables survive across blocks)
- âœ… **Editor Mode** - Execute with `Cmd/Ctrl+Shift+Enter` hotkey
- âœ… **Inline Output** - Results appear directly below code blocks
- âœ… **Error Handling** - Visual feedback for failures

### Advanced Features
- âœ… **Settings Tab** - Configure backend URL, languages, and options
- âœ… **LLM Blocks** - Execute AI prompts with ` ```llm` and ` ```agent`
- âœ… **Reading Mode** - Click Run buttons on code blocks
- âœ… **Auto Output Blocks** - Editor mode writes ` ```output` to markdown

---

## ğŸš€ Quick Start

### 1. Install Backend

```bash
cd runner
pip install -r requirements.txt
python -m uvicorn main:app --port 8000 --reload
```

Backend runs at `http://localhost:8000`

### 2. Build Plugin

```bash
cd obsidian-code-runner
npm install
npm run dev
```

### 3. Install in Obsidian

Copy `obsidian-code-runner/` to your vault's `.obsidian/plugins/` and enable in settings.

---

## ğŸ“– Usage

### Basic Execution (Reading Mode)

Create a code block:
````markdown
```python
x = 5
print(x + 2)
```
````

Switch to **Reading View** â†’ Click **â–¶ Run** â†’ Output appears below

### Editor Mode (Hotkey)

Put cursor inside any code block and press:
- **Mac:** `Cmd+Shift+Enter`
- **Windows:** `Ctrl+Shift+Enter`

An ` ```output` block is created/updated automatically.

### Kernel Mode (Persistent Sessions)

Enable "Use Kernel Mode" in settings:

````markdown
```python
x = 42
y = 10
```

```python
print(f"Sum: {x + y}")  # Works! Remembers variables
```
````

Variables persist across blocks in the same session.

### LLM Blocks

Enable "LLM / Agent blocks" in settings:

````markdown
```llm
What is the capital of France?
```
````

Click **ğŸ’¬ Run LLM** or use hotkey. (Placeholder backend - ready for OpenAI/Ollama)

---

## âš™ï¸ Settings

Access: **Settings â†’ Community Plugins â†’ Obsidian Code Runner**

- **Backend URL** - Where code execution requests are sent
- **Use Kernel Mode** - Persistent Python sessions
- **Enable Python** - Toggle Python execution
- **Enable JavaScript** - Toggle JavaScript execution
- **Enable LLM/Agent blocks** - Toggle AI prompt execution

---

## ğŸ¯ Supported Languages

| Language | Syntax | Mode | Kernel Support |
|----------|--------|------|----------------|
| Python | ` ```python` | Both | âœ… Yes |
| JavaScript | ` ```javascript` or ` ```js` | Both | âŒ No |
| LLM Prompts | ` ```llm` | Both | N/A |
| Agent Tasks | ` ```agent` | Both | N/A |

---

## ğŸ”§ Development

### Plugin Build

```bash
npm run dev     # Watch mode
npm run build   # Production build
```

### Backend Development

```bash
python -m uvicorn main:app --port 8000 --reload
```

Auto-reloads on code changes.

### Project Structure

```
obsidian-code-runner/
  â”œâ”€â”€ main.ts              # Plugin source
  â”œâ”€â”€ main.js              # Built output
  â”œâ”€â”€ styles.css           # UI styling
  â”œâ”€â”€ manifest.json        # Plugin metadata
  â””â”€â”€ package.json         # Dependencies

runner/
  â”œâ”€â”€ main.py              # FastAPI backend
  â”œâ”€â”€ requirements.txt     # Python deps
  â””â”€â”€ test_backend.py      # Tests
```

---

## ğŸ§ª Testing

### Test Kernel Mode
````markdown
```python
test_var = "Hello from kernel!"
```

```python
print(test_var)  # Should print the message
```
````

### Test Editor Mode
1. Create Python block
2. Put cursor inside
3. Press `Cmd/Ctrl+Shift+Enter`
4. Verify ` ```output` block appears

### Test LLM (Placeholder)
````markdown
```llm
Explain quantum computing in one sentence.
```
````

Should return placeholder response.

---

## ğŸ”Œ Backend API

### POST `/run`

Execute code.

**Request:**
```json
{
  "language": "python",
  "code": "print('hello')",
  "kernel": false
}
```

**Response:**
```json
{
  "stdout": "hello\n",
  "stderr": "",
  "exitCode": 0
}
```

### POST `/llm`

Execute LLM/Agent prompt (placeholder).

**Request:**
```json
{
  "mode": "llm",
  "prompt": "What is AI?"
}
```

**Response:**
```json
{
  "output": "[LLM] (placeholder)\n\nPrompt:\nWhat is AI?"
}
```

---

## ğŸ“š Documentation

- [CHANGELOG.md](CHANGELOG.md) - Development history
- [QUICK_REFERENCE.md](QUICK_REFERENCE.md) - Quick reference guide
- [runner/README.md](runner/README.md) - Backend documentation

---

## ğŸš§ Roadmap

### Coming Soon
- Real LLM integration (OpenAI/Ollama)
- Kernel restart button
- Additional languages (Ruby, Go, Rust, R)
- Rich output (images, plots, tables)
- Security improvements (Docker sandboxing)

### Future
- Per-note kernel sessions
- Cell execution controls (run all, run above/below)
- Syntax highlighting for outputs
- Mobile support (if feasible)

---

## ğŸ¤ Contributing

This is an early-stage project. Contributions welcome!

**Areas to help:**
- LLM integration (OpenAI, Anthropic, Ollama)
- Additional language support
- Security hardening
- Rich output rendering
- Documentation

---

## ğŸ“„ License

MIT

---

## ğŸ‘¤ Author

Nathan Chin

---

## ğŸ™ Acknowledgments

Built with:
- [Obsidian API](https://docs.obsidian.md/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [TypeScript](https://www.typescriptlang.org/)
- [esbuild](https://esbuild.github.io/)

---

**Status:** âœ… MVP Complete - All core features working!

Transform your Obsidian vault into a computational notebook today. ğŸš€
